{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Ensemble Learning with Python\n",
    "\n",
    "Ensemble learning methods leverage multiple base models to improve prediction performance compared to any individual estimator. In this document, we explore popular ensemble methods and implement examples using scikitâ€‘learn."
   ],
   "id": "c67549f94d213023"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Bagging\n",
    "\n",
    "Bagging (Bootstrap Aggregating) builds multiple models (typically of the same type) from different subsamples of the training dataset. The predictions are then aggregated (e.g., via majority voting) to produce the final prediction."
   ],
   "id": "d319659c2f4d873e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T14:41:52.389685Z",
     "start_time": "2025-04-10T14:41:52.339709Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create a bagging classifier with decision trees\n",
    "bagging = BaggingClassifier(\n",
    "    estimator=DecisionTreeClassifier(),\n",
    "    n_estimators=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the classifier and make predictions\n",
    "bagging.fit(X_train, y_train)\n",
    "y_pred = bagging.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Bagging Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ],
   "id": "ff3d52f7b7d9fa91",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Accuracy: 1.0\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Boosting\n",
    "\n",
    "Boosting is a sequential ensemble technique in which subsequent models attempt to correct the errors of their predecessors. One of the most popular boosting methods is AdaBoost."
   ],
   "id": "b5d5f7c1e67191d0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T14:42:32.536988Z",
     "start_time": "2025-04-10T14:42:32.389029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create an AdaBoost classifier with decision trees as base estimators\n",
    "ada = AdaBoostClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=50,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the AdaBoost classifier and make predictions\n",
    "ada.fit(X_train, y_train)\n",
    "y_pred_ada = ada.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"AdaBoost Accuracy:\", accuracy_score(y_test, y_pred_ada))\n"
   ],
   "id": "8157bab834790d37",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy: 1.0\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Voting Classifier\n",
    "\n",
    "Voting classifiers combine conceptually different machine learning models to make a final decision based on a majority vote (hard voting) or by averaging predicted probabilities (soft voting)."
   ],
   "id": "91e80e32804148a4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T14:43:00.960150Z",
     "start_time": "2025-04-10T14:43:00.720441Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "\n",
    "# Initialize base classifiers\n",
    "clf1 = LogisticRegression(random_state=42)\n",
    "clf2 = RandomForestClassifier(random_state=42)\n",
    "clf3 = SVC(probability=True, random_state=42)\n",
    "\n",
    "# Combine classifiers using hard voting\n",
    "voting = VotingClassifier(\n",
    "    estimators=[('lr', clf1), ('rf', clf2), ('svc', clf3)],\n",
    "    voting='hard'\n",
    ")\n",
    "\n",
    "# Train the voting classifier and make predictions\n",
    "voting.fit(X_train, y_train)\n",
    "y_pred_voting = voting.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Voting Classifier Accuracy:\", accuracy_score(y_test, y_pred_voting))"
   ],
   "id": "ab376cecaee0c543",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier Accuracy: 1.0\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Stacking\n",
    "\n",
    "Stacking (stacked generalization) involves training a new model (the meta-learner) to combine the predictions of several base models. The predictions of the base models serve as inputs for the meta-model."
   ],
   "id": "50047ef5b23c996e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T14:43:36.705913Z",
     "start_time": "2025-04-10T14:43:35.438415Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "# Define the base estimators for stacking\n",
    "estimators = [\n",
    "    ('lr', LogisticRegression(random_state=42)),\n",
    "    ('rf', RandomForestClassifier(random_state=42))\n",
    "]\n",
    "\n",
    "# Define a stacking classifier with a Logistic Regression meta-classifier\n",
    "stacking = StackingClassifier(\n",
    "    estimators=estimators,\n",
    "    final_estimator=LogisticRegression()\n",
    ")\n",
    "\n",
    "# Train the stacking classifier and make predictions\n",
    "stacking.fit(X_train, y_train)\n",
    "y_pred_stack = stacking.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Stacking Classifier Accuracy:\", accuracy_score(y_test, y_pred_stack))\n"
   ],
   "id": "b629c117a8ed61d0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Classifier Accuracy: 1.0\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bebe1ba08aeeb659"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
