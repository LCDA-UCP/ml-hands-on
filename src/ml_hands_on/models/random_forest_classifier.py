import numpy as np
from typing import List, Optional, Literal, Tuple
from collections import Counter

from ml_hands_on.data import Dataset
from ml_hands_on.models.decision_tree_classifier import DecisionTreeClassifier
from ml_hands_on.metrics.accuracy import accuracy


class RandomForestClassifier:
    def __init__(self,
                 n_estimators: int = 10,
                 max_features: Optional[int] = None,
                 min_sample_split: int = 2,
                 max_depth: int = 10,
                 mode: Literal['gini', 'entropy'] = 'gini',
                 seed: Optional[int] = None) -> None:
        """
        Creates a RandomForestClassifier object.

        Parameters
        ----------
        n_estimators : int
            Number of trees in the forest.
        max_features : Optional[int]
            Number of features to consider when looking for the best split.
        min_sample_split : int
            Minimum number of samples required to split a node.
        max_depth : int
            Maximum depth of the tree.
        mode : Literal['gini', 'entropy']
            Splitting criterion to use.
        seed : Optional[int]
            Seed for reproducibility.
        """
        self.n_estimators = n_estimators
        self.max_features = max_features
        self.min_sample_split = min_sample_split
        self.max_depth = max_depth
        self.mode = mode
        self.seed = seed

        self.trees: List[DecisionTreeClassifier] = []
        self.feature_indices: List[np.ndarray] = []

        if seed is not None:
            np.random.seed(seed)

    def _bootstrap_sample(self, dataset: Dataset) -> Dataset:
        """
        Performs bootstrap aggregation (bagging) by sampling the dataset with replacement.

        Parameters
        ----------
        dataset : Dataset
            The original dataset.

        Returns
        -------
        Dataset
            A new dataset generated by bootstrap sampling.
        """
        n_samples = dataset.shape()[0]
        indices = np.random.choice(n_samples, n_samples, replace=True)
        return Dataset(dataset.X[indices], dataset.y[indices], features=dataset.features, label=dataset.label)

    def _feature_bagging(self, dataset: Dataset) -> Tuple[Dataset, np.ndarray]:
        """
        Performs feature bagging by selecting a random subset of features.

        Parameters
        ----------
        dataset : Dataset
            The dataset to perform feature selection on.

        Returns
        -------
        Tuple[Dataset, np.ndarray]
            A tuple containing the dataset with selected features and the indices of selected features.
        """
        n_features = dataset.shape()[1]
        max_feats = self.max_features or int(np.sqrt(n_features))
        selected_indices = np.random.choice(n_features, max_feats, replace=False)
        new_X = dataset.X[:, selected_indices]
        new_features = [dataset.features[i] for i in selected_indices]
        new_dataset = Dataset(new_X, dataset.y, features=new_features, label=dataset.label)
        return new_dataset, selected_indices

    def fit(self, dataset: Dataset) -> 'RandomForestClassifier':
        """
        Trains the random forest classifier using bootstrap and feature bagging.

        Parameters
        ----------
        dataset : Dataset
            The dataset used to train the random forest.

        Returns
        -------
        RandomForestClassifier
            The trained classifier.
        """
        if self.seed is not None:
            np.random.seed(self.seed)

        self.trees = []
        self.feature_indices = []

        for _ in range(self.n_estimators):
            bootstrap = self._bootstrap_sample(dataset)
            boot_dataset, selected_idx = self._feature_bagging(bootstrap)
            tree = DecisionTreeClassifier(
                min_sample_split=self.min_sample_split,
                max_depth=self.max_depth,
                mode=self.mode
            )
            tree.fit(boot_dataset)
            self.trees.append(tree)
            self.feature_indices.append(selected_idx)

        return self

    def _majority_vote(self, predictions: List) -> int:
        """
        Aggregates predictions using majority voting.

        Parameters
        ----------
        predictions : List
            A list of predictions from different trees.

        Returns
        -------
        int
            The class with the majority vote.
        """
        return Counter(predictions).most_common(1)[0][0]

    def predict(self, dataset: Dataset) -> np.ndarray:
        """
        Predicts class labels for the input dataset using the trained forest.

        Parameters
        ----------
        dataset : Dataset
            Dataset to predict the labels for.

        Returns
        -------
        np.ndarray
            Array of predicted class labels.
        """
        all_preds = []

        for tree, feat_idx in zip(self.trees, self.feature_indices):
            tree_input = Dataset(dataset.X[:, feat_idx], features=[dataset.features[i] for i in feat_idx])
            preds = tree.predict(tree_input)
            all_preds.append(preds)

        all_preds = np.array(all_preds).T
        final_preds = [self._majority_vote(row) for row in all_preds]
        return np.array(final_preds)

    def score(self, dataset: Dataset) -> float:
        """
        Computes the accuracy score of the classifier on the provided dataset.

        Parameters
        ----------
        dataset : Dataset
            Dataset to evaluate the model on.

        Returns
        -------
        float
            Accuracy of the model.
        """
        preds = self.predict(dataset)
        return accuracy(dataset.y, preds)
